{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Object detection pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvCM6cvcpmdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import utils\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "# device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device=\"cpu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlwlU0vHrFTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip /content/PennFudanPed.zip -d data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVdmvP1rCSlq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ea581c4-f686-4573-f0b0-9c4166d95797"
      },
      "source": [
        "root_path='/content/data/PennFudanPed'\n",
        "glob.glob('/content/data/PennFudanPed/PNGImages')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/data/PennFudanPed/PNGImages']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ4dXI-QEiLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class PennFudanDataset(object):\n",
        "    def __init__(self, root, transforms=False):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        \n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        \n",
        "        mask = Image.open(mask_path)\n",
        "        \n",
        "        # convert the PIL Image into a numpy array\n",
        "        mask = np.array(mask)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = np.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        \n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "       \n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "        \n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img,target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5AgSvNa0fwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import transforms as T\n",
        "\n",
        "def get_transform(train=False):\n",
        "    transforms = []\n",
        "    transforms.append(T.ToTensor())\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "def my_collate(batch):\n",
        "    data = [item[0] for item in batch]\n",
        "    target = [item[1] for item in batch]\n",
        "    return [data, target]\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x39D_3d07LQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "37eed15d-3cfd-48f6-99c1-0b7cd2bb6558"
      },
      "source": [
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "dataset = PennFudanDataset('/content/data/PennFudanPed',get_transform())\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True,collate_fn=my_collate)\n",
        "images,targets = next(iter(data_loader))\n",
        "output = model(images,targets)   \n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero(Tensor input, *, Tensor out)\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(Tensor input, *, bool as_tuple)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss_classifier': tensor(0.1365, grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0024, grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0063, grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0015, grad_fn=<DivBackward0>)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07XMedVlGZ_G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "70c911dc-7bfc-4376-9380-72c8aa8c2aed"
      },
      "source": [
        "model.to(\"cpu\")\n",
        "num_epoch=1\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005,momentum=0.9, weight_decay=0.0005)\n",
        "for epoch in range(num_epoch):\n",
        "  for images,targets in data_loader:\n",
        "    optimizer.zero_grad()\n",
        "    batch_loss = model(images,targets) \n",
        "    loss=sum([l for k,l in batch_loss.items()]) \n",
        "    print(batch_loss,loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss_classifier': tensor(0.1909, grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0120, grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0441, grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0031, grad_fn=<DivBackward0>)} tensor(0.2501, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.1254, grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0063, grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0161, grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0070, grad_fn=<DivBackward0>)} tensor(0.1547, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.1076, grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0091, grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0089, grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0065, grad_fn=<DivBackward0>)} tensor(0.1321, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.0659, grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0092, grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0020, grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0064, grad_fn=<DivBackward0>)} tensor(0.0835, grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpCW79ntKF2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}